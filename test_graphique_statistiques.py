#!/usr/bin/env python3
"""
Test des statistiques et du graphique des rapports
"""

import os
import sys
import django

# Configuration Django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'equivalence.settings')
django.setup()

from dossiers.models import Dossier, DecisionCommission
from users.models import CustomUser
from django.urls import reverse
from django.test import Client

def test_statistiques_rapports():
    print("=== ğŸ§ª TEST STATISTIQUES RAPPORTS ===")
    
    # VÃ©rifier les utilisateurs
    try:
        admin = CustomUser.objects.get(username='admin')
        print(f"âœ… Admin trouvÃ©: {admin.username}")
    except CustomUser.DoesNotExist as e:
        print(f"âŒ Erreur utilisateur: {e}")
        return
    
    # Compter les dossiers par statut
    total_dossiers = Dossier.objects.count()
    dossiers_non_traites = Dossier.objects.filter(statut='non_traite').count()
    dossiers_en_cours = Dossier.objects.filter(statut='en_cours').count()
    dossiers_traites = Dossier.objects.filter(statut='traite').count()
    
    print(f"\nğŸ“Š Dossiers par statut:")
    print(f"   Total: {total_dossiers}")
    print(f"   Non traitÃ©s: {dossiers_non_traites}")
    print(f"   En cours: {dossiers_en_cours}")
    print(f"   TraitÃ©s: {dossiers_traites}")
    
    # Compter les dÃ©cisions par type
    total_evaluations = DecisionCommission.objects.count()
    evaluations_completes = DecisionCommission.objects.filter(decision='equivalence_accorder').count()
    evaluations_conditionnelles = DecisionCommission.objects.filter(decision='completement_dossier').count()
    evaluations_invitation_soutenance = DecisionCommission.objects.filter(decision='invitation_soutenance').count()
    evaluations_invitation_concours = DecisionCommission.objects.filter(decision='invitation_concours').count()
    evaluations_refusees = DecisionCommission.objects.filter(decision='non_equivalent').count()
    
    print(f"\nğŸ“Š DÃ©cisions par type:")
    print(f"   Ã‰quivalence accordÃ©e: {evaluations_completes}")
    print(f"   ComplÃ¨tement dossier: {evaluations_conditionnelles}")
    print(f"   Invitation soutenance: {evaluations_invitation_soutenance}")
    print(f"   Invitation concours: {evaluations_invitation_concours}")
    print(f"   Non Ã©quivalent: {evaluations_refusees}")
    print(f"   Total: {total_evaluations}")
    
    # Calculer la moyenne des scores
    if total_evaluations > 0:
        moyenne_score = DecisionCommission.objects.aggregate(
            moyenne=django.db.models.Avg('score_total')
        )['moyenne']
        print(f"\nğŸ“ˆ Score moyen: {moyenne_score:.1f}/100")
    else:
        print(f"\nğŸ“ˆ Score moyen: Aucune Ã©valuation")
    
    # Tester l'accÃ¨s Ã  la page des rapports
    client = Client()
    client.force_login(admin)
    
    print(f"\nğŸ”— Test accÃ¨s page rapports:")
    response = client.get(reverse('dossiers:rapports_statistiques'))
    print(f"   Status: {response.status_code} {'âœ…' if response.status_code == 200 else 'âŒ'}")
    
    if response.status_code == 200:
        content = response.content.decode('utf-8')
        
        # VÃ©rifier que les donnÃ©es sont bien affichÃ©es
        print(f"\nğŸ” VÃ©rification du contenu:")
        
        # VÃ©rifier les statistiques gÃ©nÃ©rales
        if str(total_dossiers) in content:
            print(f"   âœ… Total dossiers affichÃ©: {total_dossiers}")
        else:
            print(f"   âŒ Total dossiers non affichÃ©")
            
        if str(dossiers_non_traites) in content:
            print(f"   âœ… Non traitÃ©s affichÃ©: {dossiers_non_traites}")
        else:
            print(f"   âŒ Non traitÃ©s non affichÃ©")
            
        if str(dossiers_en_cours) in content:
            print(f"   âœ… En cours affichÃ©: {dossiers_en_cours}")
        else:
            print(f"   âŒ En cours non affichÃ©")
            
        if str(dossiers_traites) in content:
            print(f"   âœ… TraitÃ©s affichÃ©: {dossiers_traites}")
        else:
            print(f"   âŒ TraitÃ©s non affichÃ©")
        
        # VÃ©rifier les statistiques des Ã©valuations
        if str(evaluations_completes) in content:
            print(f"   âœ… Ã‰quivalence accordÃ©e affichÃ©: {evaluations_completes}")
        else:
            print(f"   âŒ Ã‰quivalence accordÃ©e non affichÃ©")
            
        if str(evaluations_conditionnelles) in content:
            print(f"   âœ… ComplÃ¨tement dossier affichÃ©: {evaluations_conditionnelles}")
        else:
            print(f"   âŒ ComplÃ¨tement dossier non affichÃ©")
            
        if str(evaluations_invitation_soutenance) in content:
            print(f"   âœ… Invitation soutenance affichÃ©: {evaluations_invitation_soutenance}")
        else:
            print(f"   âŒ Invitation soutenance non affichÃ©")
            
        if str(evaluations_invitation_concours) in content:
            print(f"   âœ… Invitation concours affichÃ©: {evaluations_invitation_concours}")
        else:
            print(f"   âŒ Invitation concours non affichÃ©")
            
        if str(evaluations_refusees) in content:
            print(f"   âœ… Non Ã©quivalent affichÃ©: {evaluations_refusees}")
        else:
            print(f"   âŒ Non Ã©quivalent non affichÃ©")
        
        # VÃ©rifier que le graphique est prÃ©sent
        if 'dossiersChart' in content:
            print(f"   âœ… Canvas graphique prÃ©sent")
        else:
            print(f"   âŒ Canvas graphique manquant")
            
        if 'Chart.js' in content:
            print(f"   âœ… Chart.js chargÃ©")
        else:
            print(f"   âŒ Chart.js non chargÃ©")
    
    print(f"\nğŸ¯ TEST TERMINÃ‰")

if __name__ == '__main__':
    test_statistiques_rapports()
